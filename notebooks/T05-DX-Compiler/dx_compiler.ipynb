{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71513b35-9e60-4c49-b3ac-45c66d7f3933",
   "metadata": {},
   "source": [
    "# DEEPX Tutorial 05 - DX-Compiler Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38306cf-27f3-454b-9794-eb4ce4ee76a3",
   "metadata": {},
   "source": [
    "In Tutorial 5, you will practice compiling classification, object detection, and segmentation models using the DX-Compiler. We will also review the guide for troubleshooting problems during compilation.\n",
    "\n",
    "The overall compilation process consists of the following steps:\n",
    "1. Obtain a pre-trained model using the PyTorch framework\n",
    "2. Convert the model to ONNX format\n",
    "3. Compile the ONNX model into DXNN format\n",
    "\n",
    "<img src=\"assets/dx-com-workflow.jpg\" style=\"max-width: 800px;\">\n",
    "\n",
    "For more details, refer to the DX-Compiler user guide ðŸ‘‰ [Download](https://developer.deepx.ai/?files=MjY0NA==)\n",
    "\n",
    "> Note: To download the User Guide, you must log in to https://developer.deepx.ai/ first.\n",
    "\n",
    "> This tutorial is based on dx-all-suite v2.1.0, released in December 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3d09f-4ccd-4589-8c7a-aa58cfd510fc",
   "metadata": {},
   "source": [
    "## 1. Compiling Image Classification Model (MobileNetV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19405ce-0615-40f3-9d1e-c2a65d39b61f",
   "metadata": {},
   "source": [
    "In this section, we will walk through the complete compilation workflow\n",
    "for an image classification model using MobileNetV2.\n",
    "\n",
    "You will learn how to:\n",
    "- Export a PyTorch model to ONNX\n",
    "- Inspect model inputs using Netron or DXTron\n",
    "- Configure preprocessing and calibration settings using JSON\n",
    "- Compile the model into DXNN format using DX-Compiler\n",
    "\n",
    "Overall process is:\n",
    "1. Export PyTorch â†’ ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cbee8-6cbc-4a78-8700-1cdc65076c4d",
   "metadata": {},
   "source": [
    "### 1.1. Export PyTorch â†’ ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b5de9-ab8a-466e-bfba-823f0fe96817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch & onnx\n",
    "!pip install --quiet torch torchvision onnx onnxsim onnxscript portpicker tqdm seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87986a8c-af2b-43f5-8860-c63f0bc14ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639e9cb-7a79-422a-93e4-700ad8fcf472",
   "metadata": {},
   "source": [
    "#### 1.1.1. Export PyTorch based MobileNetV2 model to ONNX\n",
    " - Expected result: The file `MobilenetV2.onnx` will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e609d-ba12-45e5-8f40-f0d443165d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "# Load MobileNetV2 model\n",
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Batch size must be 1\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "onnx_path = \"MobilenetV2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                      # PyTorch model object to export\n",
    "    dummy_input,                # Dummy input used for tracing (tuple is possible)\n",
    "    onnx_path,                  # Output ONNX file path\n",
    "    export_params=True,         # If True, saves model parameter (weight) into the ONNX file\n",
    "    input_names=[\"input_test\"], # Name of the ONNX model input tensor\n",
    "    output_names=[\"output\"],    # Name of the ONNX model output tensor\n",
    "    opset_version=13            # ONNX opset version (recommended: 11 ~ 21)\n",
    ")\n",
    "print(\"âœ… Save ONNX:\", onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78759017-3ffe-495f-be9a-337ac8fd0552",
   "metadata": {},
   "source": [
    "#### 1.1.2. Verify the input name and input shape using Netron or DXTron, which allows you to visualize the model topology:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9179ee-1822-48ba-a9b5-51308d6f372c",
   "metadata": {},
   "source": [
    "**Why Netron or DXTron is Important**\n",
    "\n",
    "Before creating a JSON configuration file, it is critical to understand:\n",
    "- The exact input tensor name\n",
    "- The expected input shape\n",
    "- The overall model topology\n",
    "\n",
    "Netron/DXTron provides a convenient way to inspect ONNX models and avoids\n",
    "common configuration mistakes during compilation.\n",
    "\n",
    "You can use [netron](https://netron.app) or dxtron:\n",
    " - `Note`: You can stop the `dxtron` by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a70c4e-5540-4e5e-8bac-450e0cb1ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage --no-sandbox MobilenetV2.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435e445-e87e-4706-ad8f-09d040477cb0",
   "metadata": {},
   "source": [
    "### 1.2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e037ae0-6ebd-4429-a5a6-70d56c9f818c",
   "metadata": {},
   "source": [
    "Generate a configuration file for Input/Pre-processing/Calibration of MobilenetV2.\n",
    "\n",
    "This JSON configuration file includes:\n",
    " - Input specifications\n",
    " - Calibration methods\n",
    " - Data preprocessing settings\n",
    " - Optional parameters for advanced compilation schemes\n",
    "\n",
    "Model **Input Restrictions**:\n",
    " - The batch size must be fixed to 1\n",
    " - Only a single input is supported (Multi-input will be supported in 2026)\n",
    " - Input name must exactly match ONNX model definition\n",
    "\n",
    "DX-Compiler requires strict consistency between the ONNX model and\n",
    "the JSON configuration.\n",
    "\n",
    "In the following exercises, we intentionally introduce common mistakes\n",
    "to help you understand:\n",
    "- Why input names must match exactly\n",
    "- How data layout mismatches cause shape errors\n",
    "- How to resolve these issues using preprocessing operators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e10ab-7804-4c31-b4d0-e39dd16749d9",
   "metadata": {},
   "source": [
    "#### 1.2.1. Incorrect input name case\n",
    "\n",
    "This MobilenetV2 model's input name is `input_test` as shown below:\n",
    "\n",
    "![](assets/mobilenetv2-input-name.png)\n",
    "\n",
    "Input name (`input_test` in this example) must exactly match the input name of JSON configuration.\n",
    "\n",
    "However, the following JSON configuration uses an incorrect input name (`incorrect_input_name`).\n",
    "\n",
    " - Correct input name: `input_test`\n",
    " - Current input name: `incorrect_input_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe57234-a230-4925-be56-b3a410f26ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"incorrect_input_name\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47876a-1379-4bce-be6c-6889a0cb1397",
   "metadata": {},
   "source": [
    "Let's run DX-Compiler with this wrong JSON configuration. You will encounter the following error:\n",
    "> ConfigInputError: The input name in config incorrect_input_name is not same as model input input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a8e81-84d6-4e3d-93ba-e7026b4282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01180d0a-2bfc-4ef8-8ed0-be93cbc8f755",
   "metadata": {},
   "source": [
    "#### 1.2.2. Incorrect input shape\n",
    "In the following JSON file, the input name has the correct one - `input_test`.\n",
    "\n",
    "However, even if your input shape is 1x3x224x224 (BxCxHxW), calibration image shape has 224x224x3 (HxWxC).\n",
    "\n",
    "You must change the shape of calibration image to match with 1x3x224x224 (BxCxHxW) by using `transpose` and `expandDim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655c076-5ac0-4049-bac6-b49ec98dce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc83888-67b8-4a2a-a8f2-b52787d2766e",
   "metadata": {},
   "source": [
    "Let's run DX-COM with this wrong JSON configuration. You will encounter the following error:\n",
    "\n",
    "> ConfigInputError: Config shape [1, 3, 224, 224] does not match preprocessed data shape [1, 224, 224, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac0f7-0b8e-4dce-b9be-d31586657888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57ef8e-0d99-425a-85c7-390480494f4c",
   "metadata": {},
   "source": [
    "#### 1.2.3. Add `transpose` & `expandDim` to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbd86f-e2c9-4aef-8c4d-96ed3974caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MobilenetV2.json\n",
    "{\n",
    "  \"inputs\": {\"input_test\": [1,3,224,224]},\n",
    "  \"calibration_num\": 10,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpg\",\"jpeg\",\"png\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"resize\": {\"width\": 224, \"height\": 224}},\n",
    "      {\"div\": {\"x\": 255.0}},\n",
    "      {\"normalize\": {\"mean\": [0.485,0.456,0.406], \"std\": [0.229,0.224,0.225]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd446-8d2e-4c74-98ba-1e481eb57cfc",
   "metadata": {},
   "source": [
    "### 1.3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67db40-c73e-47d2-8a45-8daee98fccd9",
   "metadata": {},
   "source": [
    "#### 1.3.1. Compile with `dx_com` to generate `.dxnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2270e5a-6b04-457b-a7b1-33a3948515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m MobilenetV2.onnx -c MobilenetV2.json -o ./ &> MobilenetV2.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cda30-d6f8-4aaf-bfd5-973c438e5978",
   "metadata": {},
   "source": [
    "#### 1.3.2. Verifying the Compiled Model\n",
    "\n",
    "The `run_model` command performs a basic inference test to verify:\n",
    "- Model correctness\n",
    "- Runtime stability\n",
    "- Successful deployment to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2907f72-5dc2-435b-9dc5-7b319a8bd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m MobilenetV2.dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b7450-bf7b-4516-acd6-cae24c5a171c",
   "metadata": {},
   "source": [
    "## 2. Compiling Object Detection Model (YOLOv7) with PPU enabled\n",
    "1. Reuse ONNX file from T03 custom YOLOv7 (Forklift & Worker)\n",
    "2. JSON configuration for Input/Pre-processing/Calibration/PPU\n",
    "3. Compile with DX-Compiler and verify .dxnn\n",
    "4. Compare latency between YOLOv7 and YOLOv7 with PPU enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264ac35-dbf1-4041-a0dc-e6698536240f",
   "metadata": {},
   "source": [
    "### 2.1. Reuse ONNX file from T03 custom YOLOv7 (Forklift & Worker)\n",
    "- Download public YOLOv7 ONNX file\n",
    "- Download the ONNX files used in the previous T03 tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26eaf3-76e9-4dee-8fdc-1211f0447193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89278628-eadc-4219-a0f4-ee9282d5c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com\n",
    "\n",
    "public_model_name = \"YOLOV7-2.onnx\"\n",
    "custom_model_name = \"yolov7-forklift-person.onnx\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(public_model_name):\n",
    "    # Download the ONNX file to detect both Forklifts and Workers\n",
    "    !wget \"sdk.deepx.ai/modelzoo/onnx/YOLOV7-2.onnx\"\n",
    "else:\n",
    "    print(f\"'{public_model_name}' already exists. Skipping download.\")\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(custom_model_name):\n",
    "    # Download the ONNX file to detect both Forklifts and Workers\n",
    "    !wget \"cs.deepx.ai/_deepx_fae_archive/dx-tutorials/yolov7-forklift-person.onnx\"\n",
    "else:\n",
    "    print(f\"'{custom_model_name}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6053d-94cc-4035-b5b2-028fc213feba",
   "metadata": {},
   "source": [
    "### 2.2. JSON configuration for Input/Pre-processing/Calibration/PPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042a7ca-0a97-449a-afd0-4514bf6b2196",
   "metadata": {},
   "source": [
    "#### 2.2.1. What is PPU?\n",
    "The PPU enables hardware-accelerated post-processing for object detection models. To use the PPU,\n",
    "define the ppu field in the configuration file with the appropriate type and parameters based on your\n",
    "model architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4da0c-c6a0-402b-95ee-cf21ac46049f",
   "metadata": {},
   "source": [
    "#### 2.2.2. PPU parameters in JSON configuration\n",
    "- **type** : `0` for anchor-based YOLO models and `1` for anchor-free YOLO models\n",
    "- **conf_thres** : Confidence threshold for detection filtering (float).\n",
    "> Note: This value is fixed during compilation and cannot be changed at runtime.\n",
    "- **num_classes** : Number of detection classes (integer)\n",
    "- **activation** : Activation function used in post-processing (commonly \"Sigmoid\" )\n",
    "- **layer** : Dictionary mapping convolution layer node names to their anchor configurations\n",
    "- Each layer entry specifies **num_anchors** : Number of anchors used in that layer\n",
    "- Example:\n",
    "  ```json\n",
    "  {\n",
    "    ... skip ...\n",
    "    \"ppu\": {\n",
    "      \"type\": 0,\n",
    "      \"conf_thres\": 0.25,\n",
    "      \"activation\": \"Sigmoid\",\n",
    "      \"num_classes\": 80,\n",
    "      \"layer\": {\n",
    "        \"Conv_245\": {\n",
    "          \"num_anchors\": 3\n",
    "        },\n",
    "        \"Conv_294\": {\n",
    "          \"num_anchors\": 3\n",
    "        },\n",
    "        \"Conv_343\": {\n",
    "          \"num_anchors\": 3\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ec6b3-0ce5-4fee-9fa9-c5788221f32d",
   "metadata": {},
   "source": [
    "#### 2.2.3. How to set PPU configurations for YOLOv7\n",
    "To configure PPU, you need to identify specific Conv operation nodes in your ONNX model:\n",
    " 1. Open your model in Netron to visualize the ONNX graph\n",
    " 2. Find the **three detection head Conv layers** and check each layer's name\n",
    "    - These Conv layers output feature maps with shape [1, num_anchors*(5+num_classes), H, W]\n",
    "    - The multiplier before (5+num_classes) is the num_anchors value you need to configure in the PPU JSON\n",
    " 3. Add ppu configuration to the JSON\n",
    "    - These Conv layers are typically followed by reshape, permute, and other post-processing operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc4b82-fb5c-48b1-9a1b-57ced0b49319",
   "metadata": {},
   "source": [
    "#### 2.2.4. Practice #1 find the three detection head Conv layers of Public YOLOv7\n",
    "\n",
    "![](assets/yolov7-class-n80-ppu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c34ba-c73f-4aea-aa36-4e70bd8d4baf",
   "metadata": {},
   "source": [
    "You can use [netron](https://netron.app) or dxtron:\n",
    " - `Note`: You can stop the `dxtron` by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad84b8a-0763-4b36-9fa5-45613aa149da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage --no-sandbox YOLOV7-2.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c96500-9051-4897-ac1a-7bf0695a337e",
   "metadata": {},
   "source": [
    "You must find the following three Conv layer names:\n",
    "  - Conv_299\n",
    "  - Conv_314\n",
    "  - Conv_329"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b15b8-7540-41bd-9e97-d2d7df6bbd80",
   "metadata": {},
   "source": [
    "#### 2.2.5. Create JSON configuration for Public YOLOv7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959ed32-70c3-47c0-8f07-9b004e99cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile YOLOv7.json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"images\": [1, 3, 640, 640]\n",
    "  },\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"calibration_num\": 100,\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e11d9d-6823-4967-a53b-333e8a98e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile YOLOv7-ppu.json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"images\": [1, 3, 640, 640]\n",
    "  },\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"calibration_num\": 100,\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  },\n",
    "  \"ppu\": {\n",
    "    \"type\": 0,\n",
    "    \"conf_thres\": 0.25,\n",
    "    \"activation\": \"Sigmoid\",\n",
    "    \"num_classes\": 80,\n",
    "    \"layer\": {\n",
    "      \"Conv_299\": {\n",
    "        \"num_anchors\": 3\n",
    "      },\n",
    "      \"Conv_314\": {\n",
    "        \"num_anchors\": 3\n",
    "      },\n",
    "      \"Conv_329\": {\n",
    "        \"num_anchors\": 3\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351546a-5a93-4848-a2a7-265b50b85080",
   "metadata": {},
   "source": [
    "#### 2.2.6. Practice #2 - find the three detection head Conv layers of Custom YOLOv7\n",
    "\n",
    "![](assets/yolov7-class-n2-ppu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfcade-a55e-4fc3-87e5-223109972620",
   "metadata": {},
   "source": [
    "You can use [netron](https://netron.app) or dxtron:\n",
    " - `Note`: You can stop the `dxtron` by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b673d-b2a6-4feb-9520-80475e82e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage --no-sandbox yolov7-forklift-person.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2acfc-39ce-43cf-857a-e9ec5ddd6a42",
   "metadata": {},
   "source": [
    "You must find the following three Conv layer names:\n",
    "  - /model.105/m.0/Conv\n",
    "  - /model.105/m.1/Conv\n",
    "  - /model.105/m.2/Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41628179-6807-405b-991f-62d081f5ea12",
   "metadata": {},
   "source": [
    "#### 2.2.7. Create JSON configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb81d9b-6ab8-4e52-8f5b-725fa02e8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile yolov7-forklift-person.json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"images\": [1, 3, 640, 640]\n",
    "  },\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"calibration_num\": 100,\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467e537-4eb3-429f-b5a8-b9eb5abb4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile yolov7-forklift-person-ppu.json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"images\": [1, 3, 640, 640]\n",
    "  },\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"calibration_num\": 100,\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  },\n",
    "  \"ppu\": {\n",
    "    \"type\": 0,\n",
    "    \"conf_thres\": 0.25,\n",
    "    \"activation\": \"Sigmoid\",\n",
    "    \"num_classes\": 2,\n",
    "    \"layer\": {\n",
    "      \"/model.105/m.0/Conv\": {\n",
    "        \"num_anchors\": 3\n",
    "      },\n",
    "      \"/model.105/m.1/Conv\": {\n",
    "        \"num_anchors\": 3\n",
    "      },\n",
    "      \"/model.105/m.2/Conv\": {\n",
    "        \"num_anchors\": 3\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672da612-a657-4aa7-ba3d-db8d3d323183",
   "metadata": {},
   "source": [
    "### 2.3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a95d74-b1df-4f2b-8550-ac75d6fd79b0",
   "metadata": {},
   "source": [
    "#### 2.3.1. Compile with `dx_com` to generate `YOLOv7.dxnn` with PPU & without PPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b6054-5b32-4d02-b073-71fbfc89cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m YOLOV7-2.onnx \\\n",
    "    -c YOLOv7.json -o output/public-yolov7 &> public-yolov7.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae9a1b-f1c3-45ac-b50f-4d62cd19d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m YOLOV7-2.onnx \\\n",
    "    -c YOLOv7-ppu.json -o output/public-yolov7-ppu &> public-yolov7-ppu.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d9d29-2a96-452c-befb-feba4b959a86",
   "metadata": {},
   "source": [
    "#### 2.3.2. Compile with `dx_com` to generate `yolov7-forklift-person.dxnn` with PPU & without PPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257281bc-94a2-4d5c-88de-499e50ac331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m yolov7-forklift-person.onnx \\\n",
    "    -c yolov7-forklift-person.json \\\n",
    "    -o output/yolov7-forklift-person &> yolov7-forklift-person.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfae04-5f1d-41e8-8c15-8a056074a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m yolov7-forklift-person.onnx \\\n",
    "    -c yolov7-forklift-person-ppu.json \\\n",
    "    -o output/yolov7-forklift-person-ppu &> yolov7-forklift-person-ppu.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8554d3-e74a-48dd-a93d-9b0dc6ecfb77",
   "metadata": {},
   "source": [
    "#### 2.3.3. Check if *.dxnn files are generated as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a442bf-3ea9-4d22-98e6-ecb4c2194f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -h output/public-yolov7* output/yolov7-forklift-person*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d8cd2-2a1d-4cd7-be9f-ce2e49fc9516",
   "metadata": {},
   "source": [
    "2.3.4. Verify *.dxnn files using run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c6ad1-8f99-4cf6-a2dd-42d2a2ee02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/public-yolov7/YOLOV7-2.dxnn -t 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ada15-f78d-4427-b283-e7715955c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/public-yolov7-ppu/YOLOV7-2.dxnn -t 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07eef3b-3413-41c6-96c7-1135401c4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/yolov7-forklift-person/yolov7-forklift-person.dxnn -t 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad7e35-9c99-4e70-bbb8-a327dd6f749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/yolov7-forklift-person-ppu/yolov7-forklift-person.dxnn -t 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c753d4-4a1c-4b23-a0f4-4cd332bf246e",
   "metadata": {},
   "source": [
    "## 3. Compiling Object Detection Model (YOLOX) with PPU enabled\n",
    "1. Download YOLOX ONNX files\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5605c01-12eb-47aa-b1af-595aac0d1c9e",
   "metadata": {},
   "source": [
    "#### 3.1. Download YOLOX-S ONNX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb809a-93b0-4ecb-920f-8dab4a964afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com\n",
    "\n",
    "# Download yoloxs model as onnx format\n",
    "model_file = \"YOLOX-S.onnx\"\n",
    "base_url = \"https://sdk.deepx.ai/modelzoo/onnx/YOLOXS-1.onnx\"\n",
    "\n",
    "# If no downloaded onnx file, download from the defined URL\n",
    "if not os.path.exists(model_file):\n",
    "    print(f\"Downloading {model_file}...\")\n",
    "    !wget -O $model_file $base_url\n",
    "else:\n",
    "    print(f\"'{model_file}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36732184-a793-4cdf-a240-09d0fd79ca52",
   "metadata": {},
   "source": [
    "### 3.2. JSON configuration for Input/Pre-processing/Calibration/PPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc90eea-c79a-408d-9c0d-3a33c35e18be",
   "metadata": {},
   "source": [
    "#### 3.2.1. PPU `type 0` vs `tpye 1`\n",
    "The PPU configuration supports the following types of object detection architectures:\n",
    "  - **Type 0 (Anchor-Based YOLO)**: Designed for models that use anchor boxes, such as YOLOv3, YOLOv4, YOLOv5, and YOLOv7.\n",
    "  - **Type 1 (Anchor-Free YOLO)**: Designed for anchor-free models, such as YOLOX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814cb8c-d5b8-4753-a813-9fa4b6a7c1b1",
   "metadata": {},
   "source": [
    "#### 3.2.2. PPU parameters in JSON configuration\n",
    "- **type** : `0` for anchor-based YOLO models and `1` for anchor-free YOLO models\n",
    "- **conf_thres** : Confidence threshold for detection filtering (float).\n",
    "> Note: This value is fixed during compilation and cannot be changed at runtime.\n",
    "- **num_classes** : Number of detection classes (integer)\n",
    "- **layer** : List of layer configurations, each containing\n",
    "  - **bbox** : Layer name that outputs bounding box coordinates\n",
    "  - **obj_conf** : Layer name that outputs object confidence scores\n",
    "  - **cls_conf** : Layer name that outputs class-wise confidence scores\n",
    "- Example:\n",
    "  ```json\n",
    "  {\n",
    "    ... skip ...\n",
    "    \"ppu\": {\n",
    "      \"type\": 1,\n",
    "      \"conf_thres\": 0.25,\n",
    "      \"num_classes\": 80,\n",
    "      \"layer\": [\n",
    "        {\n",
    "          \"bbox\": \"Conv_261\",\n",
    "          \"obj_conf\": \"Conv_262\",\n",
    "          \"cls_conf\": \"Conv_254\"\n",
    "        },\n",
    "        {\n",
    "          \"bbox\": \"Conv_282\",\n",
    "          \"obj_conf\": \"Conv_283\",\n",
    "          \"cls_conf\": \"Conv_275\"\n",
    "        },\n",
    "        {\n",
    "          \"bbox\": \"Conv_303\",\n",
    "          \"obj_conf\": \"Conv_304\",\n",
    "          \"cls_conf\": \"Conv_296\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c3091-ee61-4a91-8d71-b59229c7c05c",
   "metadata": {},
   "source": [
    "#### 3.2.3. How to set PPU configurations for YOLOX (Anchor-free model)\n",
    "\n",
    "To configure PPU, you need to identify specific Conv operation nodes in your ONNX model:\n",
    " 1. Open your model in Netron to visualize the ONNX graph\n",
    " 2. Find the three sets of `bbox`, `obj_conf`, and `cls_conf` values and check each layer's name\n",
    "    - bbox : Conv layer that outputs bounding box regression values\n",
    "    - obj_conf : Conv layer that outputs objectness confidence scores\n",
    "    - cls_conf : Conv layer that outputs class prediction scores \n",
    " 3. Add ppu configuration to the JSON\n",
    "    - These three branches run in parallel for each scale level\n",
    "    - This results in a total of 9 Conv layers (3 scales Ã— 3 branches per scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d21383-fb9a-4a14-af40-d556f733f78c",
   "metadata": {},
   "source": [
    "#### 3.2.4. Practice - find the three sets of `bbox`, `obj_conf`, and `cls_conf` values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e7ff1-72d3-4833-b066-5fcf2b9321c8",
   "metadata": {},
   "source": [
    "![](assets/yolox-class-n80-ppu.png)\n",
    "\n",
    "You can use [netron](https://netron.app) or dxtron:\n",
    " - `Note`: You can stop the `dxtron` by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1029e47-5314-479d-9248-fba702050d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage --no-sandbox YOLOX-S.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6c571-fd0f-4a2f-90e8-62ad214dbe9d",
   "metadata": {},
   "source": [
    "You must find the following nine Conv layer names:\n",
    "\n",
    "<table align=\"left\" border=\"1\" style=\"width: 550px;\">\n",
    "  <tr style=\"background-color: #D0D0D0;\"><th>Scale lvl</th><th>bbox</th><th>obj_conf</th><th>cls_conf</th></tr>\n",
    "  <tr align=\"center\"><td>80x80</td><td>Conv_261</td><td>Conv_262</td><td>Conv_254</td></tr>\n",
    "  <tr align=\"center\"><td>40x40</td><td>Conv_282</td><td>Conv_283</td><td>Conv_275</td></tr>\n",
    "  <tr align=\"center\"><td>20x20</td><td>Conv_303</td><td>Conv_304</td><td>Conv_296</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d303775-9083-4eea-9ce9-7b99387fe404",
   "metadata": {},
   "source": [
    "#### 3.2.5. Create JSON configuration for Public YOLOX-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e5ac8-a1b4-49e4-8927-f0fb92340983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile YOLOX-S.json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"images\": [1, 3, 640, 640]\n",
    "  },\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"calibration_num\": 100,\n",
    "  \"num_samples\": 1024,\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe1248-3122-4074-ace5-c17084064c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile YOLOX-S-PPU.json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"images\": [1, 3, 640, 640]\n",
    "  },\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"calibration_num\": 100,\n",
    "  \"num_samples\": 1024,\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  },\n",
    "  \"ppu\": {\n",
    "    \"type\": 1,\n",
    "    \"conf_thres\": 0.25,\n",
    "    \"num_classes\": 80,\n",
    "    \"layer\": [\n",
    "      {\n",
    "        \"bbox\": \"Conv_261\",\n",
    "        \"obj_conf\": \"Conv_262\",\n",
    "        \"cls_conf\": \"Conv_254\"\n",
    "      },\n",
    "      {\n",
    "        \"bbox\": \"Conv_282\",\n",
    "        \"obj_conf\": \"Conv_283\",\n",
    "        \"cls_conf\": \"Conv_275\"\n",
    "      },\n",
    "      {\n",
    "        \"bbox\": \"Conv_303\",\n",
    "        \"obj_conf\": \"Conv_304\",\n",
    "        \"cls_conf\": \"Conv_296\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca3d20-4e35-457d-aa65-10baa08c160d",
   "metadata": {},
   "source": [
    "### 3.3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6deaa-9958-49f3-984f-4f34b1076229",
   "metadata": {},
   "source": [
    "#### 3.3.1. Compile both YOLOX-S non-PPU and YOLOX-S PPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af804e-086b-4b50-9166-bdac14da727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m YOLOX-S.onnx -c YOLOX-S.json -o output/yolox-s &> yolox-s.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14d0fd-89a9-43dc-b3b0-5f3a0b3b04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m YOLOX-S.onnx -c YOLOX-S-PPU.json -o output/yolox-s-ppu &> yolox-s-ppu.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3dce3-9b49-491a-bbd2-f962d3af979b",
   "metadata": {},
   "source": [
    "3.3.2. Check if both **yolox-s/YOLOX-S.dxnn** and **yolox-s-ppu/YOLOX-S.dxnn** are generated as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb78b3-b642-4802-9ac3-d81cdfb23e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree output/yolox*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6b6db-6342-4eeb-871d-425454f2931d",
   "metadata": {},
   "source": [
    "#### 3.3.3. Verify them using run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b11035-43e9-4975-9c7b-90fbc708f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/yolox-s/YOLOX-S.dxnn -t 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6442c1e-f4cb-4ab9-b379-a38c9176afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/yolox-s-ppu/YOLOX-S.dxnn -t 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30a3ea-3335-4809-9b65-e0ea0479db29",
   "metadata": {},
   "source": [
    "## 4. Compiling Object Detection Model (YOLOv9-S)\n",
    "1. Export PyTorch â†’ ONNX\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133bb6f-807e-43ba-8fa9-1c55b5691280",
   "metadata": {},
   "source": [
    "### 4.1. Export PyTorch â†’ ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b15e4a-d1dd-4968-a046-e5217b12972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d78d9-00d9-4398-ba18-826abe8c5494",
   "metadata": {},
   "source": [
    "#### 4.1.1. Download `yolov9-s.pt` PyTorch model to your local system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cfb8d-1f8e-4e70-8d2a-28dfaff0c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version down to fix yolov9 version conflict issue\n",
    "!pip install torch==2.5.1 torchvision==0.20.1\n",
    "!pip install onnx onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae6a3d-d15c-4861-9794-dedd0ad4aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218e930-06a4-4be6-bf02-80f64c150f34",
   "metadata": {},
   "source": [
    "#### 4.1.2. Export the downloaded torch model to ONNX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c0d0b-c1d9-4128-8c0a-afc36fbcf020",
   "metadata": {},
   "source": [
    "Download yolov9 git repo to use `export.py` for yolov9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581e7e5-8d01-4bea-b11b-a59d2de35968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4799a-bd6a-4551-bded-724390318036",
   "metadata": {},
   "source": [
    "Export PyTorch based `yolov9-s` model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9d87a-9ce4-40c5-8722-758e356e817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov9 && python3 export.py --weights ../yolov9-s.pt \\\n",
    "                                --img-size 640 640 \\\n",
    "                                --opset 12 \\\n",
    "                                --batch-size 1 \\\n",
    "                                --include onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee5d9f-7328-49a2-bfc5-77eef6ac9b44",
   "metadata": {},
   "source": [
    "Check if `yolov9-s.onnx` file is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d853b-16f6-4d36-8419-582d7ec034b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep yolov9-s.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4186075-a2ec-4147-80a7-bac3ebe3608d",
   "metadata": {},
   "source": [
    "### 4.2. JSON c.onfiguration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e8ef2-4a03-4132-8d57-ff74cd70691a",
   "metadata": {},
   "source": [
    "You can use [netron](https://netron.app) or dxtron:\n",
    " - `Note`: You can stop the `dxtron` by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70729c9-828e-4613-bf25-a2f8797921a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage --no-sandbox yolov9-s.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c6bc5-01a4-443b-9a0c-4e0ea048acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile yolov9-s.json\n",
    "{\n",
    "  \"inputs\": {\"images\": [1,3,640,640]},\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bd2bb-83b2-4646-80ec-f0aa528bbe61",
   "metadata": {},
   "source": [
    "### 4.3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56efd1b-4e30-436a-87ff-96f6e2aef06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m yolov9-s.onnx -c yolov9-s.json -o output &> yolov9-s.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdea9f5-9c9a-42f5-a397-3b1d6a2be817",
   "metadata": {},
   "source": [
    "Check if **yolov9-s.dxnn** file is generated as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074fe4-a5b0-44e9-91f7-af519106e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep output/yolov9-s.dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2f50a-5fe9-49af-9cb9-bac6fe82eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/yolov9-s.dxnn -t 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557ccd4-f907-4711-88d3-718877a78c5d",
   "metadata": {},
   "source": [
    "## 5. Compiling Object Detection Model (YOLOv11)\n",
    "1. Download YOLOv11 ONNX files\n",
    "2. JSON configuration for Input/Pre-processing/Calibration\n",
    "3. Compile with DX-Compiler and verify .dxnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7782034-e70f-41ac-a64c-501298d80d22",
   "metadata": {},
   "source": [
    "#### 5.1. Download YOLOv11 ONNX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96ff6f-b08e-4906-9715-c5677cdf3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com\n",
    "\n",
    "# Download yolov11l/m/s models as onnx format\n",
    "model_files = [\"YOLOV11L.onnx\", \"YOLOV11M.onnx\", \"YOLOV11S.onnx\"]\n",
    "base_url = \"cs.deepx.ai/_deepx_fae_archive/dx-tutorials\"\n",
    "\n",
    "# If no downloaded onnx files, download from the defined URL\n",
    "for file_name in model_files:\n",
    "    if not os.path.exists(file_name):\n",
    "        full_url = f\"{base_url}/{file_name}\"\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        !wget $full_url\n",
    "    else:\n",
    "        print(f\"'{file_name}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd4f1c-6eba-45ef-b1ef-b832026488fc",
   "metadata": {},
   "source": [
    "#### 5.2. JSON configuration for Input/Pre-processing/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d0a6b-9a0e-4a43-902b-f9752cc78f26",
   "metadata": {},
   "source": [
    "You can use [netron](https://netron.app) or dxtron:\n",
    " - `Note`: You can stop the `dxtron` by clicking the stop button ('â– ') above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531bc02-4099-4b4b-9061-a2b776846934",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage --no-sandbox YOLOV11S.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeac0b2-d529-4951-93d1-8260702bf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile YOLOV11S.json\n",
    "{\n",
    "  \"inputs\": {\"images\": [1,3,640,640]},\n",
    "  \"calibration_num\": 100,\n",
    "  \"calibration_method\": \"ema\",\n",
    "  \"default_loader\": {\n",
    "    \"dataset_path\": \"./calibration_dataset\",\n",
    "    \"file_extensions\": [\"jpeg\",\"jpg\",\"png\",\"JPEG\"],\n",
    "    \"preprocessings\": [\n",
    "      {\"resize\": {\"mode\": \"pad\", \"size\": 640, \"pad_location\": \"edge\", \"pad_value\": [114,114,114]}},\n",
    "      {\"div\": {\"x\": 255}},\n",
    "      {\"convertColor\": {\"form\": \"BGR2RGB\"}},\n",
    "      {\"transpose\": {\"axis\": [2,0,1]}},\n",
    "      {\"expandDim\": {\"axis\": 0}}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6012d04-fbf0-427f-87d5-e3387e5d5057",
   "metadata": {},
   "source": [
    "#### 5.3. Compile with DX-Compiler and Verify .dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980aa9b-9b0c-4bac-80ec-2d3af600fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m YOLOV11S.onnx -c YOLOV11S.json -o output &> YOLOV11S.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269d22d-6066-4638-aebe-1749c27e60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep output/YOLOV11S.dxnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb296c1-f1ea-4177-91ab-ae8f64a93685",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/YOLOV11S.dxnn -t 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e1b99-96ca-412e-98bc-304e57c6c6bf",
   "metadata": {},
   "source": [
    "## 6. Compiling ViT(Vision Transformer) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca25b64-3bfd-4c60-86a7-84a3c240618a",
   "metadata": {},
   "source": [
    "https://github.com/mlfoundations/open_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85b1f2-8425-4a6a-ab36-d7de489c887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q open_clip_torch\n",
    "!pip install -q onnxruntime onnxsim onnx onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adb2be-7ccb-4195-a7e8-37582baa0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to \"dx-tutorials/dx-all-suite/dx-compiler/dx_com\"\n",
    "import os\n",
    "root_path = os.environ.get('ROOT_PATH')\n",
    "%cd $root_path/dx-all-suite/dx-compiler/dx_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea55e1-1198-44e5-968c-d12218a73ea9",
   "metadata": {},
   "source": [
    "#### 6.1. Download ONNX file from Hugging Face\n",
    "You will download `ViT-L-14-quickgelu-dfn2b` model and JSON configuration will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e4afe-1533-4e3b-bc41-9a33158a3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Download the ONNX file from Hugging Face *without hardcoding any HF URL*\n",
    "# 2) Save it as \"ViT-L-14-quickgelu-dfn2b.onnx\"\n",
    "# 3) Generate a JSON config using the existing get_config() routine\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def parse_preprocess(preprocess):\n",
    "    dx_preprocess = []\n",
    "\n",
    "    for trf in preprocess.transforms:\n",
    "        trf_name = trf.__class__.__name__\n",
    "\n",
    "        if trf_name == \"Resize\":\n",
    "            size = trf.size\n",
    "            if isinstance(size, int):\n",
    "                dx_preprocess.append({\"resize\": {\"width\": size, \"height\": size}})\n",
    "            else:\n",
    "                dx_preprocess.append({\"resize\": {\"width\": size[0], \"height\": size[1]}})\n",
    "\n",
    "        elif trf_name == \"ToTensor\":\n",
    "            dx_preprocess.append({\"div\": {\"x\": 255.0}})\n",
    "\n",
    "        elif trf_name == \"Normalize\":\n",
    "            dx_preprocess.append({\"normalize\": {\"mean\": trf.mean, \"std\": trf.std}})\n",
    "\n",
    "        # torchvision uses Lambda instead of a raw \"function\" transform\n",
    "        elif trf_name == \"Lambda\":\n",
    "            # We only support RGB conversion lambda in this demo\n",
    "            dx_preprocess.append({\"convertColor\": {\"form\": \"BGR2RGB\"}})\n",
    "\n",
    "        elif trf_name == \"CenterCrop\":\n",
    "            size = trf.size\n",
    "            if isinstance(size, int):\n",
    "                dx_preprocess.append({\"centercrop\": {\"width\": size, \"height\": size}})\n",
    "            else:\n",
    "                dx_preprocess.append({\"centercrop\": {\"width\": size[0], \"height\": size[1]}})\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported transform: {trf_name}\")\n",
    "\n",
    "    return dx_preprocess\n",
    "\n",
    "\n",
    "def get_config(img: torch.Tensor, preprocess):\n",
    "    template = {\n",
    "        \"inputs\": {\"image\": [1, 3, 224, 224]},\n",
    "        \"default_loader\": {\n",
    "            \"dataset_path\": \"calibration_dataset/\",\n",
    "            \"file_extensions\": [\"jpeg\", \"jpg\", \"png\", \"JPEG\"],\n",
    "        },\n",
    "        \"calibration_num\": 100,\n",
    "        \"calibration_method\": \"ema\",\n",
    "        \"train_batchsize\": 32,\n",
    "        \"num_samples\": 100,\n",
    "    }\n",
    "\n",
    "    _preprocess = parse_preprocess(preprocess)\n",
    "    _preprocess.append({\"transpose\": {\"axis\": [2, 0, 1]}})\n",
    "    _preprocess.append({\"expandDim\": {\"axis\": 0}})\n",
    "\n",
    "    template[\"default_loader\"][\"preprocessings\"] = _preprocess\n",
    "    return template\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "OUTPUT_DIR = Path(\"./\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verified ViT Models - You can change ViT models depending on your use cases\n",
    "# Model              | Pre-trained\n",
    "# ----------------------------------------\n",
    "# ViT-B-32-quickgelu | metaclip_fullcc\n",
    "# ViT-B-16-quickgelu | metaclip_fullcc\n",
    "# ViT-L-14-quickgelu | dfn2b (*selected)\n",
    "# ViT-B-16           | dfn2b\n",
    "# ViT-L-14           | datacomp_xl_s13b_b90k\n",
    "# ViT-B-32-256       | datacomp_s34b_b86k\n",
    "# ViT-L-14-336       | openai\n",
    "MODEL_NAME = \"ViT-L-14-quickgelu-dfn2b.onnx\"\n",
    "ONNX_DST = OUTPUT_DIR / MODEL_NAME\n",
    "JSON_DST = OUTPUT_DIR / (Path(MODEL_NAME).stem + \".json\")\n",
    "\n",
    "# This repo hosts the ONNX under \"visual/model.onnx\"\n",
    "REPO_ID = \"immich-app/ViT-L-14-quickgelu__dfn2b\"\n",
    "REPO_FILENAME = \"visual/model.onnx\"\n",
    "\n",
    "# --- 1) Download ONNX (cached by huggingface_hub) ---\n",
    "cached_path = Path(\n",
    "    hf_hub_download(\n",
    "        repo_id=REPO_ID,\n",
    "        filename=REPO_FILENAME,\n",
    "        # Set to True if you want to avoid any network calls (must be already cached).\n",
    "        # local_files_only=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Copy/rename to the filename we want to show in the tutorial\n",
    "if not ONNX_DST.exists() or ONNX_DST.stat().st_size != cached_path.stat().st_size:\n",
    "    shutil.copyfile(cached_path, ONNX_DST)\n",
    "\n",
    "print(f\"ONNX ready: {ONNX_DST} ({ONNX_DST.stat().st_size:,} bytes)\")\n",
    "\n",
    "\n",
    "# --- 2) Build a CLIP-like preprocessing pipeline (educational default) ---\n",
    "# Note: CLIP commonly uses 224x224 and the following mean/std.\n",
    "clip_mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "clip_std  = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.Lambda(lambda img: img.convert(\"RGB\")),  # recorded as \"BGR2RGB\" in our parse\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=clip_mean, std=clip_std),\n",
    "])\n",
    "\n",
    "dummy_img = torch.empty(1, 3, 224, 224)\n",
    "\n",
    "config = get_config(dummy_img, preprocess)\n",
    "with open(JSON_DST, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Config saved: {JSON_DST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525d144-9002-41e8-af71-dacb2ddf728e",
   "metadata": {},
   "source": [
    "#### 6.2. Double-check JSON Configuration & Model Topology using DXTron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec3661-b6a6-4c71-9e4f-55986dfdf38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ViT-L-14-quickgelu-dfn2b.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892eb80-4dd7-4c47-b180-81e9d1dbe68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!# Run DXTron\n",
    "!$root_path/dx-all-suite/dx-compiler/dx_tron/DXTron-2.0.0.AppImage \\\n",
    "    --no-sandbox ViT-L-14-quickgelu-dfn2b.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8781937-6a04-4ca3-a7d7-8194af8df38a",
   "metadata": {},
   "source": [
    "#### 6.3. Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e82ca-c93a-4ed2-8bf6-25bbd585e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./dx_com/dx_com -m ViT-L-14-quickgelu-dfn2b.onnx \\\n",
    "    -c ViT-L-14-quickgelu-dfn2b.json -o output &> ViT-L-14-quickgelu-dfn2b.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2ec98-d091-4847-8e1c-3a40286a47fb",
   "metadata": {},
   "source": [
    "#### 6.4. Verifying the Compiled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02904f2-e042-4d09-bbdf-62a7fab188c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!run_model -m output/ViT-L-14-quickgelu-dfn2b.dxnn -l 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526da14-1989-4227-9c69-2de17b245a74",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "- Convert PyTorch models to ONNX\n",
    "- Configure preprocessing and calibration using JSON\n",
    "- Enable PPU for object detection models\n",
    "- Compile various model architectures using DX-Compiler\n",
    "\n",
    "You are now ready to integrate DX-Compiler into your production workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318ccb0-7348-4b43-9296-79f3483055b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
